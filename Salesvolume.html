---
title: "Sales Volume Predictions"
author: "ALADEMEHIN OLUWATOMISIN RUTH"
goal: "Predicting Sales Volume"
date: "2025-10-27"
output:
  html_document:
    highlight: tango
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 4
    code_folding: hide
    toc_float: true
---


# Loading Libraries
```{r}
library(readr)
library(caret)
library(ggplot2)
library(corrplot)
```

# Introduction
	Predicting sales of four different product types and Assessing the impact of services reviews and customer reviews  on sales
    Goals:
  1. Predicting Sales
  •	Predicting sales of four different product types: PC, Laptops, Netbooks and Smartphones
  2. Assessing sales impact
  •	Assessing the impact services reviews and customer reviews have on sales of different product types


# Data Description
 We have 80 observations and 18 variables on the existing products.
 one of the variable which is "ProductType" is categorical, while the rest are numerical.
 The data was given with the project statement as an attached file.
 
 
## Loading dataset
```{r}
Sales <- read.csv("existingproductattributes2017.csv")
```



```{r}
summary(Sales)
```


```{r}
attributes(Sales)
```


```{r}
structure(Sales)
```


```{r}
names(Sales)
```

Data Description Insight;

The dataset (existingproductattributes2017.csv) includes:

1) ProductType → categorical

2) Numerical predictors → Price, ShippingWeight, Reviews (1–5 stars), Service Reviews, Product dimensions, ProfitMargin, etc.

3) Target variable → Volume (Sales Volume)


Summary inspection showed no missing values except in BestSellersRank, which was subsequently removed.

Dummy variable encoding was applied to convert the ProductType variable into model-friendly binary indicators.


# Data Preprocessing
## Checking for duplicates

```{r}
duplicated(Sales)
```


# Checking for missing values

```{r}
is.na(Sales)
```

```{r}
sum(is.na(Sales))
```

# Remove missing value variable

```{r}
Sales$BestSellersRank <- NULL
```


Missing Values Insights;

1) sum(is.na(Sales)) returned zero missing values except for BestSellersRank.

2) BestSellersRank was removed due to excessive missingness.


# Checking for outliers

```{r}
boxplot(Sales$ProductNum)
```

```{r}
boxplot(Sales$Price)
```

```{r}
boxplot(Sales$x5StarReviews)
```

```{r}
boxplot(Sales$x4StarReviews)
```


```{r}
boxplot(Sales$x3StarReviews)
```


```{r}
boxplot(Sales$x2StarReviews)
```


```{r}
boxplot(Sales$x1StarReviews)
```


```{r}
boxplot(Sales$PositiveServiceReview)
```


```{r}
boxplot(Sales$NegativeServiceReview)
```


```{r}
boxplot(Sales$Recommendproduct)
```


```{r}
boxplot(Sales$ShippingWeight)
```

```{r}
boxplot(Sales$ProductDepth)
```

```{r}
boxplot(Sales$ProductWidth)
```

```{r}
boxplot(Sales$ProductHeight)
```

```{r}
boxplot(Sales$ProfitMargin)
```

```{r}
boxplot(Sales$Volume)
```

Outliers Insight;

Boxplots revealed extreme values in:

1) Price

2) Review counts (1 to 5 stars)

3) Volume
These are expected in sales datasets and were retained since removing them may distort real behavior.



# Dummify the data

```{r}
length(unique(Sales$ProductType))
```


```{r}
newDataFrame <- dummyVars(" ~ .", data = Sales)
readyData <- data.frame(predict(newDataFrame, newdata = Sales))
```

# Correlation between the relevant independent variables and the dependent variable

```{r}
corrData <- cor(readyData)
corrData
```

Dummy Encoding

Using dummyVars(), all categorical variables were converted to numerical features to enable model processing.


```{r}
corrplot(corrData)
```



```{r}
corrplot(corrData, method = "ellipse", 
         tl.cex = 0.5,
         order = "hclust",     # groups correlated vars together
         addrect = 2,          # adds rectangles around clusters
         diag = FALSE)         # remove diagonal for clarity)
```

Correlation Analysis;

The corrplot() revealed:

1) Strong positive correlation between 5-star reviews and Volume.

2) Strong negative correlation between 1-star reviews, NegativeServiceReview, and Volume.

3) Multicollinearity was present among review variables → PLS model helps address this.

# EDA
# Univariant

```{r}
d <- ggplot(Sales, aes(ProductType))
 d + geom_bar()  +
theme(axis.text.x = element_text(angle = 50, hjust = 1))
```
Univariate Insights;

1) Accessories had the highest product count.

2) GameConsole was the least represented category.


# Bivariant

```{r}
m <- ggplot(Sales, aes(ProductType, Volume))
 m + geom_col() +
theme(axis.text.x = element_text(angle = 50, hjust = 1))
```

Insight;
  The productType with the highest Volume is the "Accessories" with an outragious outliers, while the ProductType with the least volume is "PrinterSupplies,PC and Notebook"


```{r}
f <- ggplot(Sales, aes(ProductType, Price))
 f + geom_col() +
theme(axis.text.x = element_text(angle = 50, hjust = 1))

```
Insight;
  The ProdutType with the highest price is the "PC", while productType with the least price is the "PrinterSupplies".


```{r}
g<- ggplot(Sales, aes(ProductType, PositiveServiceReview))
 g + geom_col() +
theme(axis.text.x = element_text(angle = 50, hjust = 1))
```

Insight;
  The ProductType with the best Positive service review is the "Extended Warranty" alongside with the "Accessories", while the least Positive service review are the "Netbook, printersupplies, PC, Laptop, smartphone, tablet, display and software".


```{r}
h<- ggplot(Sales, aes(ProductType, NegativeServiceReview))
 h + geom_col() +
theme(axis.text.x = element_text(angle = 50, hjust = 1))
```

Insight
  The productType with the highest Negative service review is the "Software", while the productType with the least are "Printersupplies, PC, and Netbook".
  
  

```{r}
i<- ggplot(Sales, aes(ProductType, Recommendproduct))
 i + geom_col() +
theme(axis.text.x = element_text(angle = 50, hjust = 1))
```

Insight
   The highest most recommended product Type is the "Accessories", while the least recommended is the "Netbook".
   
   

```{r}
k <- ggplot(Sales, aes(Price, Volume))
k + geom_quantile() 
```



```{r}
b <- ggplot(Sales, aes(x5StarReviews, Volume))
b + geom_quantile()   
```



```{r}
j <- ggplot(Sales, aes(x4StarReviews, Volume))
j + geom_quantile()   
```



```{r}
a <- ggplot(Sales, aes(x3StarReviews, Volume))
a + geom_quantile() 
```



```{r}
c <- ggplot(Sales, aes(x2StarReviews, Volume))
c + geom_quantile() 
```



```{r}
e <- ggplot(Sales, aes(x1StarReviews, Volume))
e + geom_quantile() 
```

Bivariate Insights

1) Accessories also had the highest sales volume, although with notable outliers.

2) PCs had the highest product price range.

3) Software had the highest negative service reviews.

4) Accessories had the highest positive recommendations.



# Multivariant

```{r warning=FALSE, message=FALSE}

#boxplot of salary and brand
ggplot (Sales,aes (x=Price, y=Volume, col=ProductType)) +
geom_point() +
theme(axis.text.x = element_text(angle = 50, hjust = 1)) +
geom_smooth() +
  labs( X = 'Price', Y = "Volume", title= "Multivariate Relationship between Price, Volume and ProductType")
```

Multivariate Insights;

A multivariate plot of Price vs Volume colored by ProductType showed:

1) Different product types respond differently to price changes.

2) Some categories have strong price–volume relationships, others much weaker.


#Predictive modeling
## Splitting the data

```{r}
set.seed(3456)
trainIndex <- createDataPartition(Sales$Volume, p = .8, 
                                  list = FALSE, 
                                  times = 1)
head(trainIndex)
```


```{r}
trainset <- Sales[trainIndex,]
testset <- Sales[-trainIndex,]
```


Predictive Modeling

The dataset was split into:

1) Training set: 80%

2) Test set: 20%


```{r}
trainset
testset
```



## Creating/building your model

```{r}
## base parameter tuning
fitControl <- trainControl(## 10-fold CV
  method = "repeatedcv",
  number = 10,
  ## repeated ten times
  repeats = 10)

```


x5StarReviews
x3StarReviews   
x2StarReviews 
x1StarReviews
PositiveServiceReview 
NegativeServiceReview


## gbm method

```{r}
gbmFit1 <- train(Volume ~ x5StarReviews + x3StarReviews + x2StarReviews + x1StarReviews + PositiveServiceReview + NegativeServiceReview, data = trainset, 
                 method = "gbm", 
                 trControl = fitControl, 
                 ## This last option is actually one
                 ## for gbm() that passes through
                 verbose = FALSE, )

gbmFit1                 
```


## Random forest model

```{r}
rfFit1 <- train(Volume ~ x5StarReviews + x3StarReviews + x2StarReviews + x1StarReviews + PositiveServiceReview + NegativeServiceReview, data = trainset, 
                 method = "rf", 
                 trControl = fitControl, 
                 ## This last option is actually one
                 ## for gbm() that passes through
                 verbose = FALSE, )

rfFit1                 
```


## Pls method

```{r warning=FALSE, message=FALSE}
plsFit1 <- train(Volume ~ x5StarReviews + x3StarReviews + x2StarReviews + x1StarReviews + PositiveServiceReview + NegativeServiceReview, data = trainset, 
                 method = "pls", 
                 trControl = fitControl, 
                 ## This last option is actually one
                 ## for pls() that passes through
                 verbose = FALSE, )

plsFit1   
```

*Final Model Selection

Even though GBM performed best, the project requirement selected PLS as the final model due to:

1) Interpretability

2) Reduced dimensionality

3) Consistency across cross-validation repeats

The final tuned PLS model was used to predict the test dataset.

```{r}
resamps_volume <- resamples(list(rf = rfFit1, gbm = gbmFit1, pls = plsFit1))
summary(resamps_volume)
```


## Building final Model using PLS

```{r warning=FALSE, message=FALSE}
plsGrid <- expand.grid(ncomp = 3)

nrow(plsGrid)

plsFitfinal <- train(Volume ~ x5StarReviews + x3StarReviews + x2StarReviews + x1StarReviews + PositiveServiceReview + NegativeServiceReview, data = trainset, 
                 method = "pls", 
                 trControl = fitControl, 
                 ## This last option is actually one
                 ## for pls() that passes through
                 verbose = FALSE,
                 tuneGrid = plsGrid)


plsFitfinal  
```


## Predicting the model

```{r}
predictplsFitfinal <- predict(plsFitfinal, newdata = testset)
predictplsFitfinal
```


```{r}
## Combine actual and predicted values into one data frame
comparison <- data.frame(
  Actual = testset$Volume,
  Predicted = predictplsFitfinal  # fixed lowercase 'p'
)

## Scatter plot using ggplot2
library(ggplot2)

ggplot(comparison, aes(x = Actual, y = Predicted)) +
  geom_point(color = "blue", alpha = 0.6) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Actual vs Predicted Volume",
    x = "Actual Volume",
    y = "Predicted Volume"
  ) +
  theme_minimal()

```

Model Evaluation

A scatter plot comparing actual vs. predicted values showed the PLS model was reasonably aligned with the diagonal reference line, indicating acceptable predictive performance.

Residuals were relatively small but larger at high sales volumes—a common behavior in heteroscedastic data.


```{r}
Sales_1<- read.csv("newproductattributes2017.csv")
```

```{r}
predictingvolume <- predict(plsFitfinal, newdata = Sales_1)
predictingvolume
```

The results were combined with the dataset and the final output table contained predicted sales volume for each new product.


```{r}
Sales_volume <- cbind(Sales_1, predictingvolume)
```


```{r}
Sales_volume
```


```{r}
#remove column by name
Sales_volume$Volume <- NULL
```


```{r}
Sales_volume
```




```{r}
# rename one column
names(Sales_volume)[names(Sales_volume) == "predictingvolume"] <- "volume"

```

```{r}
Sales_volume
```


```{r}
agg <- aggregate(Sales_volume$volume, by=list(Sales_volume$ProductType), FUN=sum)
agg
```



```{r}
colnames(agg) <- c('product', 'volume')
agg
```


```{r}
library(dplyr)
Predictedproducttype <- agg %>%
  filter(product %in% c("PC", "Laptop", "Netbook", "Smartphone"))
```


```{r}
Predictedproducttype
```



```{r}
z<- ggplot(Predictedproducttype, aes(product, volume))
 z + geom_col() +
theme(axis.text.x = element_text(angle = 50, hjust = 1))
```


Conclusion:

This project successfully built and compared three machine-learning models to predict sales volume and assess the influence of customer and service reviews.

  Key Findings

1) Higher-star reviews strongly increase sales volume.

2) Negative reviews significantly reduce volume.

3) Product types such as Accessories dominate in both number and volume.

4) GBM delivered the best predictive performance, followed by RF.

5)PLS was ultimately selected as the final model for its interpretability and generalization.

Recommendations:

1) Deploy PLS in future production systems for maximum accuracy.

2) Getting more dataset to reduce variance and improve robustness.

3) Include more features such as advertising spend or competitor pricing.
